{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import pandas as pd  # Data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np  # Linear Algebra\n",
    "from numba import jit # Just-in-Time compilation\n",
    "from tqdm import tqdm  # Process bar\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_list_filename, query_list_filename, doc_path, query_path):\n",
    "    def read_and_split(file_path, file_list, description):\n",
    "        text_split_list = []\n",
    "        for file in tqdm(file_list, desc='Reading %s' % description):\n",
    "            filename = file_path + str(file) + '.txt'\n",
    "            try:\n",
    "                with open(filename) as f:\n",
    "                    # 檔案內容切成單字列表(全小寫)\n",
    "                    text_split = [x.lower() for x in f.read().split()]\n",
    "            except:\n",
    "                    text_split = []\n",
    "            text_split_list.append(text_split)\n",
    "        return text_split_list\n",
    "\n",
    "    with open(doc_list_filename) as f:\n",
    "        doc_list = f.read().splitlines()\n",
    "    with open(query_list_filename) as f:\n",
    "        query_list = f.read().splitlines()\n",
    "    doc_text_split = read_and_split(doc_path, doc_list, 'doc')\n",
    "    query_text_split = read_and_split(query_path, query_list, 'query')\n",
    "    \n",
    "    return doc_list, query_list, doc_text_split, query_text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(doc_text_split, query_text_split):\n",
    "    index_term = Counter()\n",
    "    term_df_count = Counter()\n",
    "    doc_tf_list = []\n",
    "    for doc in tqdm(doc_text_split, desc='Count word in doc'):\n",
    "        index_term.update(doc)\n",
    "        term_df_count.update(set(doc))\n",
    "        doc_tf_list.append(Counter(doc))\n",
    "    for query in tqdm(query_text_split, desc='Update counter'):\n",
    "        index_term.update(query)\n",
    "    query_index_term = list(set([q for query in query_text_split for q in query]))\n",
    "    return index_term, query_index_term, doc_tf_list, term_df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(index_term, docs):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    r = -1\n",
    "    for d in docs:\n",
    "        r += 1\n",
    "        for term in d:\n",
    "            if term in index_term:\n",
    "                c = index_term[term]\n",
    "                row.append(r)\n",
    "                col.append(c)\n",
    "                data.append(1)\n",
    "    data = np.array(data)\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    tf_matrix = csr_matrix((data, (row, col)), shape=(len(docs), len(index_term)), dtype=np.float)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(doc_tf_matrix):\n",
    "    doc_tf_col_counter = Counter(doc_tf_matrix.tocoo().col)\n",
    "\n",
    "    df_list = []\n",
    "    for i in tqdm(range(len(doc_tf_col_counter)), desc='DF Matrix'):\n",
    "        df_list.append(doc_tf_col_counter[i])\n",
    "\n",
    "    df_matrix = np.array(df_list)\n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking\n",
    "def get_retrieved_dataf(cos_matrix, doc_list, query_list, rank):\n",
    "    retrieved_documents_list = []\n",
    "\n",
    "    for i in tqdm(range(cos_matrix.shape[0]), desc='Ranking'):\n",
    "        # np.argsort(np.argsort(Vector)) 可得到該 Value 在此 Vector 的名次(越大名次越高)\n",
    "        retrie_doc_value_dict = dict(zip(doc_list, np.argsort(np.argsort(cos_matrix[i]))))\n",
    "        # 將 (key, value) 根據 Value 進行排序，輸出 key\n",
    "        retrie_doc_sort_list = sorted(retrie_doc_value_dict.items(),\n",
    "        key = lambda retrie_doc_value_dict:retrie_doc_value_dict[1],\n",
    "        reverse = True)\n",
    "        # 將每個 key 以空格分隔輸出成 String 放至 Retrieved Documents List\n",
    "        retrieved_documents_list.append(' '.join([doc[0] for doc in retrie_doc_sort_list[:rank]]))\n",
    "    \n",
    "    # 存成 DataFrame \n",
    "    retrieved_doc_dataf = pd.DataFrame(data={\n",
    "        'Query': query_list,\n",
    "        'RetrievedDocuments': retrieved_documents_list})\n",
    "    \n",
    "    return retrieved_doc_dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def e_step(pwt, ptd, word_size, doc_size, topic_size):\n",
    "    # P(T_k|w_i, d_j)\n",
    "    ptwd = np.empty((topic_size, word_size, doc_size))\n",
    "    # each word in index_term\n",
    "    for i in range(word_size):\n",
    "        # each doc\n",
    "        for j in range(doc_size):\n",
    "            # each topic\n",
    "            for k in range(topic_size):\n",
    "                ptwd[k][i][j] = pwt[i][k] * ptd[k][j]\n",
    "        k_sum = ptwd[k][i].sum()\n",
    "        if k_sum > 0:\n",
    "            ptwd[k][i] /= k_sum\n",
    "\n",
    "    return ptwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def m_step(ptwd, cwd, word_size, doc_size, topic_size):\n",
    "    doc_text_sum = cwd.sum(axis=0)\n",
    "    \n",
    "    # create empty P(w_i|T_k) & P(T_k|d_j)\n",
    "    pwt = np.empty((word_size, topic_size))\n",
    "    ptd = np.empty((topic_size, doc_size))\n",
    "    \n",
    "    # each topic\n",
    "    for k in range(topic_size):\n",
    "        topic_wd = np.multiply(cwd, ptwd[k])\n",
    "\n",
    "        # update P(w_i|T_k)\n",
    "        wt_sum = topic_wd.sum()\n",
    "        if wt_sum > 0:\n",
    "            for i in range(word_size):\n",
    "                pwt[i][k] = topic_wd[i].sum() / wt_sum\n",
    "        else:\n",
    "            pwt[:,k] = 1 / word_size\n",
    "\n",
    "        # update P(T_k|d_j)\n",
    "        for j in range(doc_size):\n",
    "            if doc_text_sum[j] > 0:\n",
    "                ptd[k][j] = topic_wd[:,j].sum() / doc_text_sum[j]\n",
    "\n",
    "    return pwt, ptd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_preprocess_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filepath\n",
    "data_folder = 'ntust-ir-2020_hw4_v2/' # /kaggle/input/2020-information-retrieval-and-applications-hw4-v2\n",
    "doc_list_filename = data_folder + 'doc_list.txt'  # doc_list.txt filepath\n",
    "query_list_filename = data_folder + 'query_list.txt'  # query_list.txt filepath\n",
    "doc_path = data_folder + 'docs/'  # document folder path\n",
    "query_path = data_folder + 'queries/'  # query folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read document and query\n",
    "if read_preprocess_file==True:\n",
    "    doc_list = read_json('doc_list.json')\n",
    "    query_list = read_json('query_list.json')\n",
    "    doc_text_split = read_json('doc_text_split.json')\n",
    "    query_text_split = read_json('query_text_split.json')\n",
    "else:\n",
    "    doc_list, query_list, doc_text_split, query_text_split = preprocessing(doc_list_filename, query_list_filename, doc_path, query_path)\n",
    "    save_json(doc_list, 'doc_list.json')\n",
    "    save_json(query_list, 'query_list.json')\n",
    "    save_json(doc_text_split, 'doc_text_split.json')\n",
    "    save_json(query_text_split, 'query_text_split.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index term\n",
    "index_term, query_index_term, doc_tf_list, term_df_count = count_word(doc_text_split, query_text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Min-DF and Max-DF\n",
    "minDf = 5\n",
    "maxDf = 0.0007\n",
    "\n",
    "if(isinstance(minDf, float) and minDf >= 0.0 and minDf <= 1.0):\n",
    "    minDf_size = int(index_term.most_common(1)[0][1] * minDf)\n",
    "else:\n",
    "    minDf_size = minDf\n",
    "\n",
    "if(isinstance(maxDf, float) and maxDf >= 0.0 and maxDf <= 1.0):\n",
    "    maxDf_size = int(index_term.most_common(1)[0][1] * maxDf)\n",
    "else:\n",
    "    maxDf_size = min(index_term.most_common(1)[0][1], maxDf)\n",
    "\n",
    "filter_index_term = Counter(dict(filter(lambda elem: elem[0] in query_index_term or (elem[1] >= minDf_size and elem[1] <= maxDf_size)\n",
    ", term_df_count.items())))\n",
    "index_term_dict = {k: v for v, k in enumerate(list(filter_index_term.keys()))} \n",
    "\n",
    "print('index_term size:' ,len(index_term), '->', len(filter_index_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BG Model\n",
    "index_term_total = sum(filter_index_term.values())\n",
    "bg_matrix = np.empty((len(filter_index_term), 1))\n",
    "for word in filter_index_term:\n",
    "    bg_matrix[index_term_dict[word],0] = filter_index_term[word] / index_term_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size = len(filter_index_term)  # index term size\n",
    "doc_size = len(doc_list)  # number of documents\n",
    "topic_size = 8  # number of Topics\n",
    "\n",
    "# P(T_k|w_i, d_j)\n",
    "ptwd = np.empty((topic_size, word_size, doc_size))\n",
    "\n",
    "# P(w_i|T_k)\n",
    "pwt = np.random.random(size = (word_size, topic_size))\n",
    "for k in range(topic_size):\n",
    "    pwt[:,k] /= pwt[:,k].sum()\n",
    "      \n",
    "# P(T_k|d_j)\n",
    "ptd = np.full((topic_size, doc_size), 1 / topic_size)\n",
    "\n",
    "# c(w_i, d_j)\n",
    "cwd = term_frequency(index_term_dict, doc_text_split).A.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM algorithm\n",
    "maxIteration = 10\n",
    "for i in tqdm(range(maxIteration), desc='EM algorithm'):\n",
    "    ptwd = e_step(pwt, ptd, word_size, doc_size, topic_size)\n",
    "    pwt, ptd = m_step(ptwd, cwd, word_size, doc_size, topic_size)\n",
    "em_matrix = np.matmul(pwt, ptd).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLSA\n",
    "alpha = 0.6\n",
    "beta = 0.1\n",
    "sim_matrix = []\n",
    "# each query\n",
    "for query_index in tqdm(range(len(query_text_split)), desc='PLSA'):\n",
    "    query_doc_sim = []\n",
    "    # each doc\n",
    "    for doc_index in range(len(doc_text_split)):\n",
    "        sim_val = 1\n",
    "        # each word in query\n",
    "        for query in query_text_split[query_index]:\n",
    "            sim = alpha * (doc_tf_list[doc_index][query] / len(doc_text_split[doc_index]))\n",
    "            sim += beta * em_matrix[doc_index][query_index]\n",
    "            sim += (1-alpha-beta) * (filter_index_term[query] / index_term_total)\n",
    "            sim_val *= sim\n",
    "        query_doc_sim.append(sim_val)\n",
    "    sim_matrix.append(query_doc_sim)\n",
    "sim_matrix = np.array(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Retrieved Documents dataframe\n",
    "submission_df = get_retrieved_dataf(sim_matrix, doc_list, query_list, 1000)\n",
    "\n",
    "# Format filename\n",
    "submission_filename = 'submission.csv'\n",
    "\n",
    "# Submission CSV\n",
    "submission_df.to_csv(submission_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}