{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import json"
   ]
  },
  {
   "source": [
    "## Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取事先存好的 json 檔\n",
    "read_preprocessed_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'ntust-ir-2020_hw5_new'\n",
    "doc_list_filename = data_folder + '/doc_list.txt'  # doc_list 檔案路徑\n",
    "query_list_filename = data_folder + '/query_list.txt'  # query_list 檔案路徑\n",
    "doc_path = data_folder + '/docs/'  # document 檔案資料夾路徑\n",
    "query_path = data_folder + '/queries/'  # query 檔案資料夾路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_list_filename, query_list_filename, doc_path, query_path):\n",
    "    def read_and_split(file_path, file_list, description):\n",
    "        text_split_list = []\n",
    "        for file in tqdm(file_list, desc='Reading %s' % description):\n",
    "            filename = file_path + str(file) + '.txt'\n",
    "            try:\n",
    "                with open(filename) as f:\n",
    "                    # 檔案內容切成單字列表(全小寫)\n",
    "                    text_split = [x.lower() for x in f.read().split()]\n",
    "            except:\n",
    "                    text_split = []\n",
    "            text_split_list.append(text_split)\n",
    "        return text_split_list\n",
    "\n",
    "    with open(doc_list_filename) as f:\n",
    "        doc_list = f.read().splitlines()\n",
    "    with open(query_list_filename) as f:\n",
    "        query_list = f.read().splitlines()\n",
    "    doc_text_split = read_and_split(doc_path, doc_list, 'doc')\n",
    "    query_text_split = read_and_split(query_path, query_list, 'query')\n",
    "    \n",
    "    return doc_list, query_list, doc_text_split, query_text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dd(doc_text_split, query_text_split):\n",
    "    index_term = Counter()\n",
    "    term_df_count = Counter()\n",
    "    doc_tf_list = []\n",
    "    for doc in tqdm(doc_text_split, desc='Count word in doc'):\n",
    "        index_term.update(doc)\n",
    "        term_df_count.update(set(doc))\n",
    "        doc_tf_list.append(Counter(doc))\n",
    "    for query in tqdm(query_text_split, desc='Update counter'):\n",
    "        index_term.update(query)\n",
    "    query_index_term = list(set([q for query in query_text_split for q in query]))\n",
    "    return index_term, query_index_term, doc_tf_list, term_df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_preprocessed_file == True:\n",
    "    doc_list = read_json('doc_list.json')\n",
    "    query_list = read_json('query_list.json')\n",
    "    doc_text_split = read_json('doc_text_split.json')\n",
    "    query_text_split = read_json('query_text_split.json')\n",
    "else:\n",
    "    doc_list, query_list, doc_text_split, query_text_split = preprocessing(doc_list_filename, query_list_filename, doc_path, query_path)\n",
    "    save_json(doc_list, 'doc_list.json')\n",
    "    save_json(query_list, 'query_list.json')\n",
    "    save_json(doc_text_split, 'doc_text_split.json')\n",
    "    save_json(query_text_split, 'query_text_split.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_preprocessed_file == True:\n",
    "    index_term = Counter(read_json('index_term.json'))\n",
    "    query_index_term = read_json('query_index_term.json')\n",
    "    doc_tf_list = [Counter(doc) for doc in read_json('doc_tf_list.json')]\n",
    "    term_df_count = Counter(read_json('term_df_count.json'))\n",
    "else:\n",
    "    index_term, query_index_term, doc_tf_list, term_df_count = count_dd(doc_text_split, query_text_split)\n",
    "    with open('index_term.json', 'w') as f:\n",
    "        json.dump(dict(index_term), f)\n",
    "    with open('query_index_term.json', 'w') as f:\n",
    "        json.dump(query_index_term, f)\n",
    "    with open('doc_tf_list.json', 'w') as f:\n",
    "        data = [dict(doc) for doc in doc_tf_list]\n",
    "        json.dump(data, f)\n",
    "    with open('term_df_count.json', 'w') as f:\n",
    "        json.dump(dict(term_df_count), f)"
   ]
  },
  {
   "source": [
    "### TF Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(index_term, docs):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    r = -1\n",
    "    for d in tqdm(docs):\n",
    "        r += 1\n",
    "        for term in d:\n",
    "            if term in index_term:\n",
    "                c = index_term[term]\n",
    "                row.append(r)\n",
    "                col.append(c)\n",
    "                data.append(1)\n",
    "    data = np.array(data)\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    tf_matrix = csr_matrix((data, (row, col)), shape=(len(docs), len(index_term)), dtype=np.float)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Min-DF and Max-DF\n",
    "\n",
    "minDf = 7\n",
    "maxDf = 0.85\n",
    "\n",
    "if(isinstance(minDf, float) and minDf >= 0.0 and minDf <= 1.0):\n",
    "    minDf_size = int(index_term.most_common(1)[0][1] * minDf)\n",
    "else:\n",
    "    minDf_size = minDf\n",
    "\n",
    "if(isinstance(maxDf, float) and maxDf >= 0.0 and maxDf <= 1.0):\n",
    "    maxDf_size = int(index_term.most_common(1)[0][1] * maxDf)\n",
    "else:\n",
    "    maxDf_size = min(index_term.most_common(1)[0][1], maxDf)\n",
    "\n",
    "filter_index_term = Counter(dict(filter(lambda elem: elem[0] in query_index_term or (elem[1] >= minDf_size and elem[1] <= maxDf_size)\n",
    ", term_df_count.items())))\n",
    "index_term_dict = {k: v for v, k in enumerate(list(filter_index_term.keys()))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf matrix\n",
    "\n",
    "doc_tf_matrix = term_frequency(index_term_dict, doc_text_split)\n",
    "query_tf_matrix = term_frequency(index_term_dict, query_text_split)\n",
    "\n",
    "doc_tf_matrix.data = 1 + np.log(doc_tf_matrix.data)\n",
    "query_tf_matrix.data = 1 + np.log(query_tf_matrix.data)"
   ]
  },
  {
   "source": [
    "### IDF Matrix\n",
    "Inverse Document Frequency"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_requency(doc_tf_matrix):\n",
    "    doc_tf_col_counter = Counter(doc_tf_matrix.tocoo().col)\n",
    "\n",
    "    df_list = []\n",
    "    for i in range(len(doc_tf_col_counter)):\n",
    "        df_list.append(doc_tf_col_counter[i])\n",
    "\n",
    "    df_matrix = np.array(df_list)\n",
    "    return df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df matrix\n",
    "\n",
    "df_matrix = document_requency(doc_tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create idf matrix\n",
    "\n",
    "N = len(doc_text_split)\n",
    "idf_matrix = np.log((1 + N)/(1 + df_matrix)) + 1"
   ]
  },
  {
   "source": [
    "### TF-IDF Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document tfidf matrix\n",
    "doc_tfidf_matrix = doc_tf_matrix.multiply(idf_matrix)\n",
    "doc_tfidf_matrix = doc_tfidf_matrix.tocsr()\n",
    "\n",
    "# Query tfidf matrix\n",
    "query_tfidf_matrix = query_tf_matrix.multiply(idf_matrix)\n",
    "query_tfidf_matrix = query_tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "\n",
    "doc_tfidf_matrix = normalize(doc_tfidf_matrix, norm='l2')\n",
    "query_tfidf_matrix = normalize(query_tfidf_matrix, norm='l2')"
   ]
  },
  {
   "source": [
    "### Rocchio Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocchio  parameter\n",
    "\n",
    "alpha = 1\n",
    "beta = 0.5\n",
    "gamma = 0.15\n",
    "relevant_docs = 5\n",
    "non_relevant_docs = 1\n",
    "iteration = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocchio Algorithm\n",
    "\n",
    "for _ in tqdm(range(iteration), desc='Rocchio'):\n",
    "    # Cosine similarity matrix\n",
    "    cos_matrix = cosine_similarity(query_tfidf_matrix, doc_tfidf_matrix)\n",
    "    rank_matrix = np.flip(cos_matrix.argsort(axis=1)[:5000], axis=1)\n",
    "\n",
    "    for i in range(rank_matrix.shape[0]):\n",
    "        rele_doc_vec = doc_tfidf_matrix[rank_matrix[i,:relevant_docs]].mean(axis=0)\n",
    "        non_rele_vec = doc_tfidf_matrix[rank_matrix[i,:-non_relevant_docs]].mean(axis=0)\n",
    "        query_tfidf_matrix[i] = alpha * query_tfidf_matrix[i] + beta * rele_doc_vec - gamma * non_rele_vec"
   ]
  },
  {
   "source": [
    "### Rank\n",
    "1. 根據剛剛的 Cosine similarity Matrix，可以把每個 Query 與所有 Document 的相似程度做排名，並把排名結果以 Document 檔名依序列出，存成一個 Retrieved Documents List。\n",
    "2. 把 Query List 和 Retrieved Documents List 建成一個 DatafFrame，輸出成 CSV。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_dataf(cos_matrix, doc_list, query_list):\n",
    "    retrieved_documents_list = []\n",
    "\n",
    "    for i in tqdm(range(cos_matrix.shape[0]), desc='Ranking'):\n",
    "        # np.argsort(np.argsort(Vector)) 可得到該 Value 在此 Vector 的名次(越大名次越高)\n",
    "        retrie_doc_value_dict = dict(zip(doc_list, np.argsort(np.argsort(cos_matrix[i]))))\n",
    "        # 將 (key, value) 根據 Value 進行排序，輸出 key\n",
    "        retrie_doc_sort_list = sorted(retrie_doc_value_dict.items(),\n",
    "        key = lambda retrie_doc_value_dict:retrie_doc_value_dict[1],\n",
    "        reverse = True)\n",
    "        # 將每個 key 以空格分隔輸出成 String 放至 Retrieved Documents List\n",
    "        retrieved_documents_list.append(' '.join([doc[0] for doc in retrie_doc_sort_list[:5000]]))\n",
    "    \n",
    "    # 存成 DataFrame \n",
    "    retrieved_doc_dataf = pd.DataFrame(data={\n",
    "        'Query': query_list,\n",
    "        'RetrievedDocuments': retrieved_documents_list})\n",
    "    \n",
    "    return retrieved_doc_dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = get_retrieved_dataf(cos_matrix, doc_list, query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current date and time\n",
    "date_time = datetime.now().strftime(\"%m%d%H%M\")\n",
    "\n",
    "submission_filename = 'hw5_%s_a%s_b%s_g%s_rd%s_nrd%s_it%s_hdf%s_ldf%s.csv' % (date_time, alpha, beta, gamma, relevant_docs, non_relevant_docs, iteration, maxDf, minDf)\n",
    "submission_message = 'alpha=%s, beta=%s, gamma=%s, relevant_docs=%s, non_relevant_docs=%s, iteration=%s, maxDf=%s, minDf=%s' % (alpha, beta, gamma, relevant_docs, non_relevant_docs, iteration, maxDf, minDf)\n",
    "\n",
    "# Submission CSV\n",
    "submission_df.to_csv(submission_filename, index=False)"
   ]
  },
  {
   "source": [
    "### Kaggle Submit API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Submit API\n",
    "\n",
    "import os\n",
    "os.system('cmd /c \\\"kaggle competitions submit -c 2020-information-retrieval-and-applications-hw5 -f %s -m \\\"%s\\\"\\\"' % (submission_filename, submission_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}