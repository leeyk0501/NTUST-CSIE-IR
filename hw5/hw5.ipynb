{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 5 - Query Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "source": [
    "### Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc_list_filename, query_list_filename, doc_path, query_path):\n",
    "    def read_and_split(file_path, file_list, description):\n",
    "        text_split_list = []\n",
    "        for file in tqdm(file_list, desc='Reading %s' % description):\n",
    "            filename = file_path + str(file) + '.txt'\n",
    "            try:\n",
    "                with open(filename) as f:\n",
    "                    # 檔案內容切成單字列表(全小寫)\n",
    "                    text_split = [x.lower() for x in f.read().split()]\n",
    "            except:\n",
    "                    text_split = []\n",
    "            text_split_list.append(text_split)\n",
    "        return text_split_list\n",
    "\n",
    "    with open(doc_list_filename) as f:\n",
    "        doc_list = f.read().splitlines()\n",
    "    with open(query_list_filename) as f:\n",
    "        query_list = f.read().splitlines()\n",
    "    doc_text_split = read_and_split(doc_path, doc_list, 'doc')\n",
    "    query_text_split = read_and_split(query_path, query_list, 'query')\n",
    "    \n",
    "    return doc_list, query_list, doc_text_split, query_text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "source": [
    "### TF Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dd(doc_text_split, query_text_split):\n",
    "    index_term = Counter()\n",
    "    term_df_count = Counter()\n",
    "    doc_tf_list = []\n",
    "    for doc in tqdm(doc_text_split, desc='Count word in doc'):\n",
    "        index_term.update(doc)\n",
    "        term_df_count.update(set(doc))\n",
    "        doc_tf_list.append(Counter(doc))\n",
    "    for query in tqdm(query_text_split, desc='Update counter'):\n",
    "        index_term.update(query)\n",
    "    query_index_term = list(set([q for query in query_text_split for q in query]))\n",
    "    return index_term, query_index_term, doc_tf_list, term_df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(index_term, docs, description):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    r = -1\n",
    "    for d in tqdm(docs, desc='%s TF Matrix' % description):\n",
    "        r += 1\n",
    "        for term in d:\n",
    "            if term in index_term:\n",
    "                c = index_term[term]\n",
    "                row.append(r)\n",
    "                col.append(c)\n",
    "                data.append(1)\n",
    "    data = np.array(data)\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    tf_matrix = csr_matrix((data, (row, col)), shape=(len(docs), len(index_term)), dtype=np.float)\n",
    "    return tf_matrix"
   ]
  },
  {
   "source": [
    "### IDF Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_requency(doc_tf_matrix):\n",
    "    doc_tf_col_counter = Counter(doc_tf_matrix.tocoo().col)\n",
    "\n",
    "    df_list = []\n",
    "    for i in tqdm(range(len(doc_tf_col_counter)), desc='DF Matrix'):\n",
    "        df_list.append(doc_tf_col_counter[i])\n",
    "\n",
    "    df_matrix = np.array(df_list)\n",
    "    return df_matrix"
   ]
  },
  {
   "source": [
    "### TF-IDF Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(tf_matrix, idf_matrix):\n",
    "    tfidf_matrix = tf_matrix.multiply(idf_matrix).tocsr()\n",
    "    # Normalize\n",
    "    tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "source": [
    "### Best Match Models (BM25)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Match Models (BM25)\n",
    "def BM25(k1, k3, b, doc_text_split, query_text_split, doc_tf_matrix, query_tf_matrix, idf_matrix, index_term_dict):\n",
    "\n",
    "    avg_doclen = sum([len(doc) for doc in doc_text_split]) / len(doc_text_split)\n",
    "\n",
    "    bm_sim_matrix = []\n",
    "    for query_index in tqdm(range(len(query_text_split)), desc='BM25'):\n",
    "        query_doc_sim = []\n",
    "        for doc_index in range(len(doc_text_split)):\n",
    "            sim = 0\n",
    "            for query in query_text_split[query_index]:\n",
    "                if query in doc_text_split[doc_index]:\n",
    "                    word_index = index_term_dict[query]\n",
    "                    # BM25\n",
    "                    v1 = ((k1 + 1) * doc_tf_matrix[doc_index, word_index])/(k1 * ((1-b) + b*len(doc_text_split[doc_index])/avg_doclen) + doc_tf_matrix[doc_index, word_index])\n",
    "                    v2 = ((k3 + 1) * query_tf_matrix[query_index, word_index])/(k3 + query_tf_matrix[query_index, word_index])\n",
    "                    v3 = idf_matrix[word_index]\n",
    "                    sim += (v1 * v2 * v3)\n",
    "            query_doc_sim.append(sim)\n",
    "        bm_sim_matrix.append(query_doc_sim)\n",
    "    bm_sim_matrix = np.array(bm_sim_matrix)\n",
    "    bm_sim_matrix[bm_sim_matrix==0] = 1.\n",
    "    return bm_sim_matrix"
   ]
  },
  {
   "source": [
    "### Rocchio Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocchio Algorithm\n",
    "\n",
    "def rocchio(doc_tfidf_matrix, query_tfidf_matrix, additional_matrix):\n",
    "    rocchio_query_tfidf_matrix = query_tfidf_matrix.copy()\n",
    "    \n",
    "    for _ in tqdm(range(iteration), desc='Rocchio'):\n",
    "        # Cosine similarity matrix\n",
    "        cos_matrix = cosine_similarity(rocchio_query_tfidf_matrix, doc_tfidf_matrix)\n",
    "\n",
    "        # Combine some models result to achieve a good performance\n",
    "        sim_matrix = np.multiply(cos_matrix, additional_matrix)\n",
    "\n",
    "        # Get relevant document\n",
    "        # rank_matrix = np.flip(sim_matrix.argsort(axis=1)[:5000], axis=1)\n",
    "        rank_matrix = np.flip(sim_matrix.argsort(axis=1), axis=1)\n",
    "        all_non_relevant_docs = non_relevant_docs + (rank_matrix.shape[1]-5000)\n",
    "\n",
    "        for i in range(rank_matrix.shape[0]):\n",
    "            # relevant document vector\n",
    "            rele_doc_vec = doc_tfidf_matrix[rank_matrix[i,:relevant_docs]].mean(axis=0)\n",
    "            # non relevant document vector\n",
    "            # non_rele_vec = doc_tfidf_matrix[rank_matrix[i,:-non_relevant_docs]].mean(axis=0)\n",
    "            non_rele_vec = doc_tfidf_matrix[rank_matrix[i,:-all_non_relevant_docs]].mean(axis=0)\n",
    "            # move some distance toward the centroid of the relevant documents, \n",
    "            # and move some distance away from the centroid of the non relevant documents\n",
    "            rocchio_query_tfidf_matrix[i] = alpha * rocchio_query_tfidf_matrix[i] + beta * rele_doc_vec - gamma * non_rele_vec\n",
    "    return cos_matrix"
   ]
  },
  {
   "source": [
    "### Ranking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_dataf(cos_matrix, doc_list, query_list):\n",
    "    retrieved_documents_list = []\n",
    "\n",
    "    for i in tqdm(range(cos_matrix.shape[0]), desc='Ranking'):\n",
    "        # np.argsort(np.argsort(Vector)) 可得到該 Value 在此 Vector 的名次(越大名次越高)\n",
    "        retrie_doc_value_dict = dict(zip(doc_list, np.argsort(np.argsort(cos_matrix[i]))))\n",
    "        # 將 (key, value) 根據 Value 進行排序，輸出 key\n",
    "        retrie_doc_sort_list = sorted(retrie_doc_value_dict.items(),\n",
    "        key = lambda retrie_doc_value_dict:retrie_doc_value_dict[1],\n",
    "        reverse = True)\n",
    "        # 將每個 key 以空格分隔輸出成 String 放至 Retrieved Documents List\n",
    "        retrieved_documents_list.append(' '.join([doc[0] for doc in retrie_doc_sort_list[:5000]]))\n",
    "    \n",
    "    # 存成 DataFrame \n",
    "    retrieved_doc_dataf = pd.DataFrame(data={\n",
    "        'Query': query_list,\n",
    "        'RetrievedDocuments': retrieved_documents_list})\n",
    "    \n",
    "    return retrieved_doc_dataf"
   ]
  },
  {
   "source": [
    "### Program"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取事先存好的 json 檔\n",
    "read_preprocessed_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "data_folder = 'ntust-ir-2020_hw5_new'\n",
    "doc_list_filename = data_folder + '/doc_list.txt'  # doc_list 檔案路徑\n",
    "query_list_filename = data_folder + '/query_list.txt'  # query_list 檔案路徑\n",
    "doc_path = data_folder + '/docs/'  # document 檔案資料夾路徑\n",
    "query_path = data_folder + '/queries/'  # query 檔案資料夾路徑\n",
    "\n",
    "# Read doc and query file\n",
    "if read_preprocessed_file == True:\n",
    "    doc_list = read_json('doc_list.json')\n",
    "    query_list = read_json('query_list.json')\n",
    "    doc_text_split = read_json('doc_text_split.json')\n",
    "    query_text_split = read_json('query_text_split.json')\n",
    "else:\n",
    "    doc_list, query_list, doc_text_split, query_text_split = preprocessing(doc_list_filename, query_list_filename, doc_path, query_path)\n",
    "    save_json(doc_list, 'doc_list.json')\n",
    "    save_json(query_list, 'query_list.json')\n",
    "    save_json(doc_text_split, 'doc_text_split.json')\n",
    "    save_json(query_text_split, 'query_text_split.json')\n",
    "\n",
    "# \n",
    "if read_preprocessed_file == True:\n",
    "    index_term = Counter(read_json('index_term.json'))\n",
    "    query_index_term = read_json('query_index_term.json')\n",
    "    doc_tf_list = [Counter(doc) for doc in read_json('doc_tf_list.json')]\n",
    "    term_df_count = Counter(read_json('term_df_count.json'))\n",
    "else:\n",
    "    index_term, query_index_term, doc_tf_list, term_df_count = count_dd(doc_text_split, query_text_split)\n",
    "    with open('index_term.json', 'w') as f:\n",
    "        json.dump(dict(index_term), f)\n",
    "    with open('query_index_term.json', 'w') as f:\n",
    "        json.dump(query_index_term, f)\n",
    "    with open('doc_tf_list.json', 'w') as f:\n",
    "        data = [dict(doc) for doc in doc_tf_list]\n",
    "        json.dump(data, f)\n",
    "    with open('term_df_count.json', 'w') as f:\n",
    "        json.dump(dict(term_df_count), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Min-DF and Max-DF\n",
    "minDf = 10\n",
    "maxDf = 0.25\n",
    "\n",
    "if(isinstance(minDf, float) and minDf >= 0.0 and minDf <= 1.0):\n",
    "    minDf_size = int(index_term.most_common(1)[0][1] * minDf)\n",
    "else:\n",
    "    minDf_size = minDf\n",
    "\n",
    "if(isinstance(maxDf, float) and maxDf >= 0.0 and maxDf <= 1.0):\n",
    "    maxDf_size = int(index_term.most_common(1)[0][1] * maxDf)\n",
    "else:\n",
    "    maxDf_size = min(index_term.most_common(1)[0][1], maxDf)\n",
    "\n",
    "filter_index_term = Counter(dict(filter(lambda elem: elem[0] in query_index_term or (elem[1] >= minDf_size and elem[1] <= maxDf_size)\n",
    ", term_df_count.items())))\n",
    "index_term_dict = {k: v for v, k in enumerate(list(filter_index_term.keys()))} \n",
    "\n",
    "# Create tf matrix\n",
    "doc_tf_matrix = term_frequency(index_term_dict, doc_text_split, 'Doc')\n",
    "query_tf_matrix = term_frequency(index_term_dict, query_text_split, 'Query')\n",
    "doc_tf_matrix.data = 1 + np.log(doc_tf_matrix.data)\n",
    "query_tf_matrix.data = 1 + np.log(query_tf_matrix.data)\n",
    "\n",
    "# Create df matrix\n",
    "df_matrix = document_requency(doc_tf_matrix)\n",
    "\n",
    "# Create idf matrix\n",
    "idf_matrix = np.log((1 + len(doc_text_split))/(1 + df_matrix)) + 1\n",
    "\n",
    "# Document tfidf matrix\n",
    "doc_tfidf_matrix = TF_IDF(doc_tf_matrix, idf_matrix)\n",
    "\n",
    "# Query tfidf matrix\n",
    "query_tfidf_matrix = TF_IDF(query_tf_matrix, idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "k1 = 1.8\n",
    "k3 = 1500\n",
    "b = 0.85\n",
    "\n",
    "if read_preprocessed_file == True:\n",
    "    bm_sim_matrix = np.load('bm_sim_matrix.npy')\n",
    "else:\n",
    "    bm_sim_matrix = BM25(k1, k3, b, doc_text_split, query_text_split, doc_tf_matrix, query_tf_matrix, idf_matrix, index_term_dict)\n",
    "    np.save('bm_sim_matrix.npy', bm_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocchio\n",
    "alpha = 1\n",
    "beta = 0.5\n",
    "gamma = 0.15\n",
    "relevant_docs = 5\n",
    "non_relevant_docs = 1\n",
    "iteration = 7\n",
    "\n",
    "cos_matrix = rocchio(doc_tfidf_matrix, query_tfidf_matrix, bm_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Retrieved Documents dataframe\n",
    "submission_df = get_retrieved_dataf(cos_matrix, doc_list, query_list)\n",
    "\n",
    "# Current date and time\n",
    "date_time = datetime.now().strftime(\"%m%d%H%M\")\n",
    "\n",
    "# Format filename\n",
    "submission_filename = 'hw5_%s_a%s_b%s_g%s_rd%s_nrd%s_it%s_hdf%s_ldf%s.csv' % (date_time, alpha, beta, gamma, relevant_docs, non_relevant_docs, iteration, maxDf, minDf)\n",
    "submission_message = 'alpha=%s, beta=%s, gamma=%s, relevant_docs=%s, non_relevant_docs=%s, iteration=%s, maxDf=%s, minDf=%s, BM25(k1=%s, k3=%s, b=%s)' % (alpha, beta, gamma, relevant_docs, non_relevant_docs, iteration, maxDf, minDf, k1, k3, b)\n",
    "\n",
    "# Submission CSV\n",
    "submission_df.to_csv(submission_filename, index=False)"
   ]
  },
  {
   "source": [
    "### Kaggle Submit API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Submit API\n",
    "\n",
    "import os\n",
    "os.system('cmd /c \\\"kaggle competitions submit -c 2020-information-retrieval-and-applications-hw5 -f %s -m \\\"%s\\\"\\\"' % (submission_filename, submission_message))"
   ]
  }
 ]
}